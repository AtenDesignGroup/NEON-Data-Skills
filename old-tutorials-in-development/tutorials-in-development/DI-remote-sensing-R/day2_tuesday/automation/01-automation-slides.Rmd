---
title: "Introduction to automation"
output:
  ioslides_presentation:
    highlight: pygments
    widescreen: yes
---

## 

<center>
"Reproducibilty is actually all about being as lazy as possible!"

-- Hadley Wickham (via
[Twitter](https://twitter.com/hadleywickham/status/598532170160873472),
2015-05-03)
</center>


## Prerequisites

- Some previous exposure to R Markdown is recommended. If you have not used R Markdown before,  this lesson
might seem too advanced as it provides ways to deal with annoyances you can face when you try to use it within a large project. 

- On the other hand, if you have never used R Markdown before this is a good opportunity to learn 
efficient practices right away.

## Goals

- Main goal: learn to use R Markdown to more effectively manage a research project involving: 
    - multiple sources of raw data;
    - that are combined to create multiple intermediate datasets that are themselves combined to generate figures and tables. We will show you some tips and tricks that will make your life easier with R
Markdown.
- Learn to structure the components of your research project to make it easy to automate and connect together.
- Learn to automate the repeatable steps in your research (using Make and Makefiles). 
- Introduction to tools commonly used in research software projects supporting automation, such as Travis CI.

**Start small**: You don't need to adopt all of these practices at once. Start with only one dataset, one figure first, and later learn how to use Make, and then Travis. 

## Toolkit level and skills

- None of these tools are exceedingly hard to learn or to grasp, but trying to learn everything at once might feel overwhelming. 

- In the process of learning these tools, you will learn transferable skills useful in other aspects of your research. For example: 
    - Interested in writing a package in R? Many of the advice and tools included
    in this lesson will greatly help you get started.
    - Want or need to learn to use Linux or UNIX environments? The lessons on Make and Travis CI will provide an opportunity
    to learn some basics of UNIX and Linux. You can then build on these skills to be able to run robust and efficient  analysis of large datasets (e.g., set up runs on 
    an HPC cluster).
    

# Challenges of the reproducible workflow

- Disorganisation and lack of planning <br/>
- Keeping the analysis, research ideas and code all in sync as you progress with your work.
- Developing your analysis at the same time as writing your
manuscript and refining your ideas, adjusting the aim of your paper, deciding on the data you are going to include, etc.

R Markdown allows you to mix code and prose, which is wonderful and very 
powerful but can be difficult to manage if you don't have a good plan to get
organized. Make allows you to keep things in sync. If you change one component of your research projects, you can easily check how other components are affected and update other outputs. 

# Challenge 1: Code organisation 
Structuring and *software engineering* your code to make it easier to maintain, reuse and support automation.
    
## Solution: Modularisation 

<center>
![Functionalize all the things!](img/functionalize_all_the_things.jpg)
</center>

### Outline

Demonstrate writing functions to approach different tasks: clean version of your data, your figures, your tables and your manuscript. 

- **Why?** Having all the code in your script structured into functions will greatly facilitate the maintenance of your script as it forces it to be organized, easier to read, reuse and extend. 

- **Additional benefits:**
    1. Modularity.
    1. Allows for using fewer (global) variables.
    1. Makes it easier to document (allows for self-documented code).
    1. Testing - makes it possible to write unit tests.

#### Benefit 1. Modularity

- By breaking down your manuscript into functions, you will have blocks of code
that can be reused and interact with each other. The dependencies between different bits of code will also be clearer and more explicit.

- Modularity allows you to avoid repeating yourself, and you will be able to re-use the functions you create for other projects more easily than if your paper only contains scripts (compare: a house of cards vs. house made of Lego).
    
#### Benefit 2. Fewer variables to worry about...

...so you can focus on the important stuff!

- **Avoid having to track large number of variables:** If your script is one big chunk of code without functions, you are likely to create and accumulate many variables. You will have to worry about avoiding name conflicts among all these variables that store intermediate versions of your datasets but you will not eventually need in 
your analysis.
    - By learning how to use functions, passing arguments, local variables and returning values you can focus on the important stuff: the inputs and the outputs of your workflow.

- **Keep track of dependencies easily:** Using functions to clean or transform data, calculate 
results, generate figures will help you see dependencies more clearly, understand how changes in different components may affect each other and so on.


#### Benefit 3. Easier documentation

- Ideally, your code should be written so that it's easy to understand and your
intentions are clear (it is sometimes called *self-documented code*). 
    - However, what might seem clear to you now might be clear as mud 6 months 
    from now or even 3 weeks from now. 
    - Other times, it might not seem very efficient to refactor a piece of code 
    to make it clearer, and you end up with a piece of code that works but is 
    clunky.
    - If you thrive on geekiness and/or nerdiness you might end up over-engineering a part of your code and make it more difficult to understand a 
    few weeks later. 
    
- In all of these situations, and even if you think your code is clear and 
simple, it's important that you **document** your code and the functions it includes, for 
your collaborators, and your future self.

##### Documenting whole scripts vs. documenting functions

- If all your analysis is made up of scripts, not organised in functions, with pieces that are repeated in
multiple parts of your document, things can get out of hand pretty quickly. 

- Not only is it more difficult to maintain because you will have to find and 
replace the thing that you need to change in multiple places of your code, but 
maintaining and updating documentation is also challenging. 
    - Do you also duplicate your comments where you duplicate parts of your 
    scripts? 
    - How do you keep the duplicated comments in sync? 

- Re-organizing your scripts into functions (or organizing your analysis in 
functions from the beginning) will allow you to explicitly document the dataset
or the parameters on which your function, and therefore your results, depends
on.

##### Tips for documenting your code

- Easiest way: add comments around your functions to explicitly indicate the 
purpose of each function, what the arguments are supposed to be (class and 
format) and the kind of output you will get from it.
- Document the input (arguments) for your function (type, etc.).
- Document the output (what your function returns).

##### Documentation, kicked up a notch

[roxygen](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html) is a format that allows for documentation of functions, and can easily 
be converted into file formats used by R documentation. 

- Writing for roxygen is not very different from simple comments, you just 
need to add some keywords to define what will end up in the different 
sections of the help files. 
- This is not a strict requirement, and will it not make your analysis more 
reproducible, but it will be useful down the road if you think you will 
convert your manuscript into a package (more on this in a sec). 
- RStudio makes it easy to write roxygen. 
    - Once you have started writing a function, in the menu choose `Code > 
    Insert Roxygen Skeleton` or type `Ctrl + Alt + Shift + R` on your 
    keyboard.

#### Benefit 4: Testing

- When you start writing a lot of code for your paper, it becomes easier to
introduce bugs. 
    - For example, if your analysis relies on data that gets updated often, you 
    may want to make sure that all the columns are there, and that they don't 
    include values that they should not.

- If these issues break something in your analysis, you might be able to find it
easily, but more often than not, these issues might produce subtle differences
in your results that you may not be able to detect.

- If all your code is made up of functions, then you can control the input and
test for the output. It is something that would be difficult, if not impossible,
to do if all your analysis is in the form of a long script. In short, you can test each function individually. 

##### Testing, kicked up a notch

The 
[testthat](https://cran.r-project.org/web/packages/testthat/index.htm) package 
provides a powerful and easy-to-use framework to build tests for your functions.

# Challenge 2: Organizing your files

## File organization

![example of file organization we will use today](img/file_organization.png)

## File organization for this lesson

- `data-raw`: the original data, you shouldn't edit or otherwise alter any of
the files in this folder.
- `data-output`: intermediate datasets that will be generated by the
analysis. 
      - We write them into CSV files so we could share them with collaborators. 
      - If it took a long time to generate the files, we may want to also 
      use them for our analysis. For this example, they are small and can be 
      recreated every time.
- `fig`: the folder where we can store the figures used in the manuscript.
- `R`: our R code (the functions)
    - Often easier to keep the prose separated from the code. 
    - If you have a lot of code (and/or manuscript is long), it's easier 
    to navigate.
- `tests`: the code to test that our functions are behaving properly and that
all our data is included in the analysis.

## What's next?

Today, we are going to work on functionalizing a `knitr` document that is more
complex than what we have seen so far but not quite as complex as a "real"
research document could look like.

Let's take a look at `example-manuscript` folder...
